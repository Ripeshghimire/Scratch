{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn \n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InputEmbedding(nn.Module):\n",
    "    def __init__(self,d_model:int,vocab_size:int):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding = nn.Embedding(vocab_size,d_model)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        self.embedding(x) * math.sqrt(self.d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNormalization(nn.Module):\n",
    "\n",
    "    def __init__(self, features: int, eps:float=10**-6) -> None:\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        self.alpha = nn.Parameter(torch.ones(features)) # alpha is a learnable parameter\n",
    "        self.bias = nn.Parameter(torch.zeros(features)) # bias is a learnable parameter\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (batch, seq_len, hidden_size)\n",
    "         # Keep the dimension for broadcasting\n",
    "        mean = x.mean(dim = -1, keepdim = True) # (batch, seq_len, 1)\n",
    "        # Keep the dimension for broadcasting\n",
    "        std = x.std(dim = -1, keepdim = True) # (batch, seq_len, 1)\n",
    "        # eps is to prevent dividing by zero or when std is very small\n",
    "        return self.alpha * (x - mean) / (std + self.eps) + self.bias\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting accelerate>=0.26.0\n",
      "  Downloading accelerate-1.2.1-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: numpy<3.0.0,>=1.17 in /Users/ripeshghimire/coding/ResearchPaper/env/lib/python3.11/site-packages (from accelerate>=0.26.0) (2.1.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/ripeshghimire/coding/ResearchPaper/env/lib/python3.11/site-packages (from accelerate>=0.26.0) (24.2)\n",
      "Requirement already satisfied: psutil in /Users/ripeshghimire/coding/ResearchPaper/env/lib/python3.11/site-packages (from accelerate>=0.26.0) (6.1.0)\n",
      "Requirement already satisfied: pyyaml in /Users/ripeshghimire/coding/ResearchPaper/env/lib/python3.11/site-packages (from accelerate>=0.26.0) (6.0.2)\n",
      "Requirement already satisfied: torch>=1.10.0 in /Users/ripeshghimire/coding/ResearchPaper/env/lib/python3.11/site-packages (from accelerate>=0.26.0) (2.5.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.21.0 in /Users/ripeshghimire/coding/ResearchPaper/env/lib/python3.11/site-packages (from accelerate>=0.26.0) (0.27.0)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /Users/ripeshghimire/coding/ResearchPaper/env/lib/python3.11/site-packages (from accelerate>=0.26.0) (0.4.5)\n",
      "Requirement already satisfied: filelock in /Users/ripeshghimire/coding/ResearchPaper/env/lib/python3.11/site-packages (from huggingface-hub>=0.21.0->accelerate>=0.26.0) (3.16.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /Users/ripeshghimire/coding/ResearchPaper/env/lib/python3.11/site-packages (from huggingface-hub>=0.21.0->accelerate>=0.26.0) (2024.9.0)\n",
      "Requirement already satisfied: requests in /Users/ripeshghimire/coding/ResearchPaper/env/lib/python3.11/site-packages (from huggingface-hub>=0.21.0->accelerate>=0.26.0) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /Users/ripeshghimire/coding/ResearchPaper/env/lib/python3.11/site-packages (from huggingface-hub>=0.21.0->accelerate>=0.26.0) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/ripeshghimire/coding/ResearchPaper/env/lib/python3.11/site-packages (from huggingface-hub>=0.21.0->accelerate>=0.26.0) (4.12.2)\n",
      "Requirement already satisfied: networkx in /Users/ripeshghimire/coding/ResearchPaper/env/lib/python3.11/site-packages (from torch>=1.10.0->accelerate>=0.26.0) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /Users/ripeshghimire/coding/ResearchPaper/env/lib/python3.11/site-packages (from torch>=1.10.0->accelerate>=0.26.0) (3.1.4)\n",
      "Requirement already satisfied: sympy==1.13.1 in /Users/ripeshghimire/coding/ResearchPaper/env/lib/python3.11/site-packages (from torch>=1.10.0->accelerate>=0.26.0) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Users/ripeshghimire/coding/ResearchPaper/env/lib/python3.11/site-packages (from sympy==1.13.1->torch>=1.10.0->accelerate>=0.26.0) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/ripeshghimire/coding/ResearchPaper/env/lib/python3.11/site-packages (from jinja2->torch>=1.10.0->accelerate>=0.26.0) (3.0.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/ripeshghimire/coding/ResearchPaper/env/lib/python3.11/site-packages (from requests->huggingface-hub>=0.21.0->accelerate>=0.26.0) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/ripeshghimire/coding/ResearchPaper/env/lib/python3.11/site-packages (from requests->huggingface-hub>=0.21.0->accelerate>=0.26.0) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/ripeshghimire/coding/ResearchPaper/env/lib/python3.11/site-packages (from requests->huggingface-hub>=0.21.0->accelerate>=0.26.0) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/ripeshghimire/coding/ResearchPaper/env/lib/python3.11/site-packages (from requests->huggingface-hub>=0.21.0->accelerate>=0.26.0) (2024.8.30)\n",
      "Downloading accelerate-1.2.1-py3-none-any.whl (336 kB)\n",
      "Installing collected packages: accelerate\n",
      "Successfully installed accelerate-1.2.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install 'accelerate>=0.26.0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting text from PDF...\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import pdfplumber\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, Trainer, TrainingArguments\n",
    "from datasets import Dataset\n",
    "\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    \"\"\"Extracts text from a PDF file.\"\"\"\n",
    "    with pdfplumber.open(pdf_path) as pdf:\n",
    "        text = \"\".join([page.extract_text() for page in pdf.pages])\n",
    "    return text\n",
    "\n",
    "def chunk_text(text, max_chunk_size=512):\n",
    "    \"\"\"Splits text into smaller chunks fitting the model's token limit.\"\"\"\n",
    "    words = text.split()\n",
    "    chunks = []\n",
    "    current_chunk = []\n",
    "    current_length = 0\n",
    "    \n",
    "    for word in words:\n",
    "        if current_length + len(word) + 1 > max_chunk_size:\n",
    "            chunks.append(\" \".join(current_chunk))\n",
    "            current_chunk = []\n",
    "            current_length = 0\n",
    "        current_chunk.append(word)\n",
    "        current_length += len(word) + 1\n",
    "    \n",
    "    if current_chunk:\n",
    "        chunks.append(\" \".join(current_chunk))\n",
    "    return chunks\n",
    "\n",
    "def prepare_dataset(chunks):\n",
    "    \"\"\"Creates a dataset from text chunks.\"\"\"\n",
    "    return Dataset.from_dict({\"text\": chunks})\n",
    "\n",
    "def fine_tune_model(pdf_path, model_name=\"gpt2\", output_dir=\"fine_tuned_model\"):\n",
    "    \"\"\"Fine-tunes a model on text extracted from a PDF.\"\"\"\n",
    "    print(\"Extracting text from PDF...\")\n",
    "    text = extract_text_from_pdf(pdf_path)\n",
    "    \n",
    "    print(\"Chunking text...\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    \n",
    "    # Set pad_token if it doesn't exist\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "    \n",
    "    chunks = chunk_text(text, max_chunk_size=tokenizer.model_max_length)\n",
    "    \n",
    "    print(\"Preparing dataset...\")\n",
    "    dataset = prepare_dataset(chunks)\n",
    "    \n",
    "    def tokenize_function(examples):\n",
    "        return tokenizer(\n",
    "            examples[\"text\"],\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            max_length=tokenizer.model_max_length,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "    \n",
    "    tokenized_dataset = dataset.map(\n",
    "        tokenize_function,\n",
    "        batched=True,\n",
    "        remove_columns=[\"text\"]\n",
    "    )\n",
    "    \n",
    "    print(\"Loading model...\")\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "    \n",
    "    # Explicitly set to CPU\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"Using CPU device\")\n",
    "    model.to(device)\n",
    "    \n",
    "    print(\"Setting up training arguments...\")\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=output_dir,\n",
    "        overwrite_output_dir=True,\n",
    "        num_train_epochs=3,\n",
    "        per_device_train_batch_size=2,\n",
    "        gradient_accumulation_steps=8,\n",
    "        save_steps=100,\n",
    "        save_total_limit=2,\n",
    "        logging_dir=f\"{output_dir}/logs\",\n",
    "        logging_steps=10,\n",
    "        learning_rate=5e-5,\n",
    "        weight_decay=0.01,\n",
    "        warmup_steps=100,\n",
    "        # Explicitly set no_cuda to True to prevent GPU usage\n",
    "        no_cuda=True,\n",
    "        # Remove all mixed precision settings\n",
    "        fp16=None,\n",
    "        fp16_opt_level=None,\n",
    "        fp16_backend=None,\n",
    "        fp16_full_eval=None,\n",
    "        bf16=None,\n",
    "        tf32=None,\n",
    "        push_to_hub=False,\n",
    "        max_grad_norm=0.5,\n",
    "        remove_unused_columns=False,\n",
    "        gradient_checkpointing=True,\n",
    "        dataloader_pin_memory=False\n",
    "    )\n",
    "    \n",
    "    print(\"Initializing trainer...\")\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=tokenized_dataset,\n",
    "        tokenizer=tokenizer,\n",
    "    )\n",
    "    \n",
    "    print(\"Fine-tuning the model...\")\n",
    "    trainer.train()\n",
    "    \n",
    "    print(\"Saving the model...\")\n",
    "    trainer.save_model(output_dir)\n",
    "    print(f\"Model saved to {output_dir}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    pdf_path = \"/Users/ripeshghimire/coding/ResearchPaper/Andhra_Pradesh.pdf\"\n",
    "    model_name = \"gpt2\"\n",
    "    output_dir = \"fine_tuned_model\"\n",
    "    \n",
    "    if not os.path.exists(pdf_path):\n",
    "        print(f\"Error: File {pdf_path} does not exist.\")\n",
    "    else:\n",
    "        fine_tune_model(pdf_path, model_name=model_name, output_dir=output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
