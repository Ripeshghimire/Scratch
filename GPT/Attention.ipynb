{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n1.self attention is a mechanism in transformers that is used to compute more efficient input representations by allowing each position in a sequence to interact with and weigh\\nthe importance of all other positions within the same sequence \\n2. The \"self\" in self attention ?\\n    >ability to compute attention weights by relating different positions within a single input sequence.\\n    >assesses and learns the relationship and dependencies between various parts of the inputs itself such as word in a senctence or pixels in an image \\nwhat is the main goal of self-attention ?\\n    >compute a context vector for each input element, that combines information from all other input elements. \\n    >importance or contribution of each input element for computing is determined by attention weights\\n    >when computing attention weights are calculated with respect to input element, and all other inputs. \\ncontext vector: \\ncontext vector is an embedding that contains information about all other inputs elements. It can be interpreted as an enriched embedding vector\\nwhat is the use of context vector? \\n    > to create enriched representation of each elements in an input sequence like a sentence by incorporation information from all other elements in the sequence\\n    >helps to understand realtionsship and relevance of words in a sentecne to each other. \\n    >Later we will add trainable weights that help an LLM learng to consturct these context vectors so that they are relevant for the LLM to generate the next token\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "1.self attention is a mechanism in transformers that is used to compute more efficient input representations by allowing each position in a sequence to interact with and weigh\n",
    "the importance of all other positions within the same sequence \n",
    "2. The \"self\" in self attention ?\n",
    "    >ability to compute attention weights by relating different positions within a single input sequence.\n",
    "    >assesses and learns the relationship and dependencies between various parts of the inputs itself such as word in a senctence or pixels in an image \n",
    "what is the main goal of self-attention ?\n",
    "    >compute a context vector for each input element, that combines information from all other input elements. \n",
    "    >importance or contribution of each input element for computing is determined by attention weights\n",
    "    >when computing attention weights are calculated with respect to input element, and all other inputs. \n",
    "context vector: \n",
    "context vector is an embedding that contains information about all other inputs elements. It can be interpreted as an enriched embedding vector\n",
    "what is the use of context vector? \n",
    "    > to create enriched representation of each elements in an input sequence like a sentence by incorporation information from all other elements in the sequence\n",
    "    >helps to understand realtionsship and relevance of words in a sentecne to each other. \n",
    "    >Later we will add trainable weights that help an LLM learng to consturct these context vectors so that they are relevant for the LLM to generate the next token\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nAttention mechanism the text generating decoder part of the network can acesss all input token selectively this mean that some input tokens are more important than others for generating \\na given output token \\nThis means that some input toekns are more importaant than others for genrating a given output token .\\n\\nself in self attention refers to the mechanism ability to compute attention weights by relating different positions within a single input sequence . \\nLearns the relationship amd dependencies between various parts of the input itself, such as sequence to sequence model where the attention might be between an input sequence and \\noutput sequence\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Attention mechanism the text generating decoder part of the network can acesss all input token selectively this mean that some input tokens are more important than others for generating \n",
    "a given output token \n",
    "This means that some input toekns are more importaant than others for genrating a given output token .\n",
    "\n",
    "self in self attention refers to the mechanism ability to compute attention weights by relating different positions within a single input sequence . \n",
    "Learns the relationship amd dependencies between various parts of the input itself, such as sequence to sequence model where the attention might be between an input sequence and \n",
    "output sequence\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "inputs = torch.tensor(\n",
    "  [[0.43, 0.15, 0.89], # Your     (x^1)\n",
    "   [0.55, 0.87, 0.66], # journey  (x^2)\n",
    "   [0.57, 0.85, 0.64], # starts   (x^3)\n",
    "   [0.22, 0.58, 0.33], # with     (x^4)\n",
    "   [0.77, 0.25, 0.10], # one      (x^5)\n",
    "   [0.05, 0.80, 0.55]] # step     (x^6)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865])\n"
     ]
    }
   ],
   "source": [
    "query = inputs[1] # query vector using the inputs first index \n",
    "attention = torch.empty(inputs.shape[0]) #initializing an empty torch object to store the attention weights\n",
    "for i,x_i in enumerate(inputs):\n",
    "    attention[i] = torch.dot(x_i,query) #calculating dot product to calculate attention weights\n",
    "print(attention)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.9544)\n"
     ]
    }
   ],
   "source": [
    "# a dot product is essentially just a concise way of multiplying two vectors element-wise and then summing the products\n",
    "res = 0\n",
    "for idx,element in enumerate(inputs[0]):\n",
    "    res += inputs[0][idx] * query[idx]\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalizing the vector "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.1455, 0.2278, 0.2249, 0.1285, 0.1077, 0.1656])\n",
      "sum tensor(1.0000)\n"
     ]
    }
   ],
   "source": [
    "attention_weights_tmp = attention / attention.sum()\n",
    "print(attention_weights_tmp)\n",
    "print(\"sum\",attention_weights_tmp.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581])\n",
      "sum tensor(1.)\n"
     ]
    }
   ],
   "source": [
    "attention_weights_2 = torch.softmax(attention,dim=0)\n",
    "print(attention_weights_2)\n",
    "print(\"sum\",attention_weights_2.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now that we computed the normalized attention weights we are ready for the final step \n",
    "#calculating the context vector by multiplying the embedded input tokens, with the correspoding attention weights and then summing the resulting vectors \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.4300, 0.1500, 0.8900])\n",
      "tensor([0.5500, 0.8700, 0.6600])\n",
      "tensor([0.5700, 0.8500, 0.6400])\n",
      "tensor([0.2200, 0.5800, 0.3300])\n",
      "tensor([0.7700, 0.2500, 0.1000])\n",
      "tensor([0.0500, 0.8000, 0.5500])\n",
      "tensor([0.4419, 0.6515, 0.5683])\n"
     ]
    }
   ],
   "source": [
    "query = inputs[1]\n",
    "context_vector = torch.zeros(query.shape)\n",
    "for idx,x_i in enumerate(inputs):\n",
    "    print(x_i)\n",
    "    context_vector += x_i * attention_weights_2[idx]\n",
    "print(context_vector)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n1.Compute attention scores\\n2.Compute attention weights \\n3.Compute context vectors\\n'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "1.Compute attention scores\n",
    "2.Compute attention weights \n",
    "3.Compute context vectors\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.9995, 0.9544, 0.9422, 0.4753, 0.4576, 0.6310],\n",
      "        [0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865],\n",
      "        [0.9422, 1.4754, 1.4570, 0.8296, 0.7154, 1.0605],\n",
      "        [0.4753, 0.8434, 0.8296, 0.4937, 0.3474, 0.6565],\n",
      "        [0.4576, 0.7070, 0.7154, 0.3474, 0.6654, 0.2935],\n",
      "        [0.6310, 1.0865, 1.0605, 0.6565, 0.2935, 0.9450]])\n"
     ]
    }
   ],
   "source": [
    "attn_scores = torch.empty(6,6)\n",
    "for i,x_i in enumerate(inputs):\n",
    "    for j,x_j in enumerate(inputs):\n",
    "        attn_scores[i,j] = torch.dot(x_i,x_j)\n",
    "print(attn_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.9995, 0.9544, 0.9422, 0.4753, 0.4576, 0.6310],\n",
       "        [0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865],\n",
       "        [0.9422, 1.4754, 1.4570, 0.8296, 0.7154, 1.0605],\n",
       "        [0.4753, 0.8434, 0.8296, 0.4937, 0.3474, 0.6565],\n",
       "        [0.4576, 0.7070, 0.7154, 0.3474, 0.6654, 0.2935],\n",
       "        [0.6310, 1.0865, 1.0605, 0.6565, 0.2935, 0.9450]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "attn_scores = inputs @ inputs.T\n",
    "attn_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2098, 0.2006, 0.1981, 0.1242, 0.1220, 0.1452],\n",
      "        [0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581],\n",
      "        [0.1390, 0.2369, 0.2326, 0.1242, 0.1108, 0.1565],\n",
      "        [0.1435, 0.2074, 0.2046, 0.1462, 0.1263, 0.1720],\n",
      "        [0.1526, 0.1958, 0.1975, 0.1367, 0.1879, 0.1295],\n",
      "        [0.1385, 0.2184, 0.2128, 0.1420, 0.0988, 0.1896]])\n"
     ]
    }
   ],
   "source": [
    "attn_weights = torch.softmax(attn_scores,dim =1 )\n",
    "print(attn_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_context_vector = attn_weights @ inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.4421, 0.5931, 0.5790],\n",
       "        [0.4419, 0.6515, 0.5683],\n",
       "        [0.4431, 0.6496, 0.5671],\n",
       "        [0.4304, 0.6298, 0.5510],\n",
       "        [0.4671, 0.5910, 0.5266],\n",
       "        [0.4177, 0.6503, 0.5645]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_context_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_2  = inputs[1]\n",
    "d_in = inputs.shape[1]\n",
    "d_out = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(123)\n",
    "W_query = nn.Parameter(torch.rand(d_in,d_out),requires_grad=False)\n",
    "W_key = nn.Parameter(torch.rand(d_in,d_out),requires_grad=False)\n",
    "W_value = nn.Parameter(torch.rand(d_in,d_out),requires_grad=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.4306, 1.4551])\n"
     ]
    }
   ],
   "source": [
    "query_2 = x_2 @ W_query\n",
    "key_2 = x_2 @ W_key\n",
    "value_2 = x_2 @ W_value\n",
    "print(query_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keys.shape torch.Size([6, 2])\n",
      "value.shape torch.Size([6, 2])\n"
     ]
    }
   ],
   "source": [
    "keys = inputs @ W_key \n",
    "values = inputs@ W_value\n",
    "print(\"keys.shape\",keys.shape)\n",
    "print(\"value.shape\",values.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.8524)\n"
     ]
    }
   ],
   "source": [
    "key_2 = keys[1]\n",
    "attn_score_22 = query_2.dot(key_2)\n",
    "print(attn_score_22)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.2705, 1.8524, 1.8111, 1.0795, 0.5577, 1.5440])\n"
     ]
    }
   ],
   "source": [
    "attn_score_2 = query_2 @ keys.T \n",
    "print(attn_score_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6, 2])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keys.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keys.shape[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.1500, 0.2264, 0.2199, 0.1311, 0.0906, 0.1820])\n",
      "torch.Size([6])\n"
     ]
    }
   ],
   "source": [
    "d_k = keys.shape[-1]\n",
    "attn_weights_2 = torch.softmax(attn_score_2 / d_k ** 0.5,dim=-1)\n",
    "print(attn_weights_2)\n",
    "print(attn_weights_2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.3061, 0.8210])\n"
     ]
    }
   ],
   "source": [
    "context_vector_2 = attn_weights_2 @ values \n",
    "print(context_vector_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn \n",
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self,d_in,d_out):\n",
    "        super().__init__()\n",
    "        self.d_out = d_out \n",
    "        self.W_query = nn.Parameter(torch.rand(d_in,d_out))\n",
    "        self.W_key  = nn.Parameter(torch.rand(d_in,d_out))\n",
    "        self.W_value = nn.Parameter(torch.rand(d_in,d_out))\n",
    "    def forward(self,x):\n",
    "        keys = x @ self.W_key\n",
    "        query = x @ self.W_query\n",
    "        value = x @ self.W_value\n",
    "        attn_scores = query @ keys.T \n",
    "        attn_weights = torch.softmax(attn_scores/keys.shape[-1]**0.5, dim=-1)\n",
    "        context_vector = attn_weights @ value\n",
    "        return context_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1771, 0.4980],\n",
      "        [0.1794, 0.5031],\n",
      "        [0.1819, 0.5064]], grad_fn=<MmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "sa_v1 = SelfAttention(d_in,d_out)\n",
    "print(sa_v1(inputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn \n",
    "class SelfAttention_v2(nn.Module):\n",
    "    def __init__(self,d_in,d_out,qkv_bias= False):\n",
    "        super().__init__()\n",
    "        self.d_out = d_out \n",
    "        self.W_query = nn.Linear(d_in,d_out,bias = False)\n",
    "        self.W_key  = nn.Linear(d_in,d_out,bias = False)\n",
    "        self.W_value = nn.Linear(d_in,d_out,bias= False)\n",
    "    def forward(self,x):\n",
    "        keys =  self.W_key(x)\n",
    "        query =  self.W_query(x)\n",
    "        value =  self.W_value(x)\n",
    "        attn_scores = query @ keys.T \n",
    "        attn_weights = torch.softmax(attn_scores/keys.shape[-1]**0.5, dim=-1)\n",
    "        context_vector = attn_weights @ value\n",
    "        return context_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0739,  0.0713],\n",
      "        [-0.0748,  0.0703],\n",
      "        [-0.0749,  0.0702],\n",
      "        [-0.0760,  0.0685],\n",
      "        [-0.0763,  0.0679],\n",
      "        [-0.0754,  0.0693]], grad_fn=<MmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(789)\n",
    "sa_v2 = SelfAttention_v2(d_in,d_out)\n",
    "print(sa_v2(inputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
