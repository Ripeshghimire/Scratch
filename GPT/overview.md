
1.Why should we build our own LLMs ? 
we learn a lot while building our own custom LLMs that helps to understand from ground up of how does the llm works and while training a model it helps to understand its mechanic and limitation
custom made llms that are trained on custom data perform well than the generalized LLM like chatgpt 
also it equips with the knowledge of pretraining and finetuning the existing llm for our usecase


What is the process of creating an LLM? 

There are two stages of creating an LLM :
PreTraining:
It refers to the training of the model in a large diverse dataset  to develop broad understanding of the language 
Pre-Trained model serves a foundation resources that can be further refined through  finetuning

FineTuning
After the model is pre-trained on  a large diverse dataset it is finetuned using smaller dataset for everyusecases that the model requires
for eg: the model can be finetuned to code by providing a dataset of coding

What is autoregressive model ? 
Autoregressive model are the model that uses the previous output as inputs to determine the future predictions 

what is the use of predicting the next token ? 
The use of predicing the next to key to gpt is predicts the next token according to the user queries

What are the stages of Building an LLM? 


2.Working with Text data 
    Preparing text for large language model training 
    Splitting text into word and subword token
    Byte Pair Encoding as a more advanced way of tokenizing text 
    Sampling training exmaple with a sliding window approach 
    Converting tokens into vectors that feed into a large language model 

Why do we need to understand word embedding ? 


What is the main problem of deep neural networks and LLM?


Does various data require various embedding ? 

how do you define the embedding than ? how do you store it? how do we know if the embedding is of video,audio or text? 
